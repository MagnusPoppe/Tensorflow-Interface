{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tutorials i TensorFlow fra Kieth Downing\n",
    "## Sett 2, Autoencoder.\n",
    "\n",
    "Dette eksempelet er et neuralt nett som baserer seg på bits. Den skal få inn bits, så gjennom et hemmlig lag bestående av færre noder enn inputverdier skal den finne tilbake til bits som var i input.\n",
    "\n",
    "Autoencoder er her laget som en egen klasse. Denne klassen kan bygge det neurale nettet med gitte parametere som spesifiserer hvordan nettet skal se ut. \n",
    "\n",
    "Keith downings forklarende tekst:\n",
    "\n",
    "We can extend the basic approach in tfex8 (tutor1.py) to handle \n",
    "1. A 3-layered neural network\n",
    "2. A collection of cases to be learned.  \n",
    "\n",
    "This is a specialized neural network designed to solve one type of classification problem: converting an input string, through a single hidden layer, to a copy of itself on the output end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppbygging av nettet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HENTET FRA autoencoder():\n",
    "def __init__(self,nh=3,lr=.1):\n",
    "    self.cases = TFT.gen_all_one_hot_cases(2**nh)\n",
    "    self.learning_rate = lr\n",
    "    self.num_hiddens = nh\n",
    "    self.build_neural_network(nh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her brukes det to variabler som input for metoden. Disse to skal spesifisere nettet. \n",
    "\n",
    "```python\n",
    "\tnh = 3   # number of hidden nodes in the hidden layer\n",
    "\tlr = 0.2 # learning rate used with gradient decent.\n",
    "```\n",
    "\n",
    "Casene hentes fra en genereringsmetode laget av downing. Denne lager bare bit-strenger. Altså sekvenser med bits. Her kommer sekvensene i lengde 2*nh. De hver sekvens er altså dobbelt så lage som antall hidden nodes.\n",
    "\n",
    "Hvis det er tre noder i det hemmelige laget, kommer input i form av:\n",
    "```\n",
    "\t011001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYGGING AV DET NEURALE NETTET:\n",
    "Under kommer forklaring av koden i metoden \"build_neural_network()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network(self,nh):\n",
    "    ios = 2**nh  # ios = input- and output-layer size\n",
    "    self.w1 = tf.Variable(np.random.uniform(-.1,.1,size=(ios,nh)),name='Weights-1')  # first weight array\n",
    "    self.w2 = tf.Variable(np.random.uniform(-.1,.1,size=(nh,ios)),name='Weights-2') # second weight array\n",
    "    self.b1 = tf.Variable(np.random.uniform(-.1,.1,size=nh),name='Bias-1')  # First bias vector\n",
    "    self.b2 = tf.Variable(np.random.uniform(-.1,.1,size=ios),name='Bias-2')  # Second bias vector\n",
    "    self.input = tf.placeholder(tf.float64,shape=(1,ios),name='Input')\n",
    "    self.target = tf.placeholder(tf.float64,shape=(1,ios),name='Target')\n",
    "    self.hidden = tf.sigmoid(tf.matmul(self.input,self.w1) + self.b1,name=\"Hiddens\")\n",
    "    self.output = tf.sigmoid(tf.matmul(self.hidden,self.w2) + self.b2, name = \"Outputs\")\n",
    "    self.error = tf.reduce_mean(tf.square(self.target - self.output),name='MSE')\n",
    "    self.predictor = self.output  # Simple prediction runs will request the value of outputs\n",
    "    # Defining the training operator\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "    self.trainer = optimizer.minimize(self.error,name='Backprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det bortgjemte laget skal alltid være halve størrelsen av input - og output lagene. \n",
    "```\n",
    "ios = 2**nh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Han definerer 2 forskjellige sett med vekter her. Det må to sett til fordi det er et hidden-layer mellom input og output. All kommunikasjon skjer gjennom vektmatrisene. Vektmatrisene kobler sammen de forskjellige lagene. Det er viktig at de forskjellige vektmatrisene er riktig størrelse. \n",
    "\n",
    "Input er 5 i bredde, mens hidden layer er 3. Dette betyr at vektmatrise 1 (w1) har dimensjonene 5x3. \n",
    "\n",
    "Hidden layer er bredde 3, mens output er lengde 5. Dette betyr at vektmatrise 2 (w2) må være på dimensjonene 3x5\n",
    "\n",
    "```\n",
    "\tself.w1 = tf.Variable(np.random.uniform(-.1,.1,size=(ios,nh)),name='Weights-1')  # first weight array\n",
    "\tself.w2 = tf.Variable(np.random.uniform(-.1,.1,size=(nh,ios)),name='Weights-2') # second weight array\n",
    "   \n",
    "```\n",
    "\n",
    "![modell](https://github.com/MagnusPoppe/Tensorflow-Interface/blob/master/3-node%20Modell.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det produserers så to bias. Bias betyr \"partiskhet\" og er viktig i å \"overtale\" nettet til å velge riktig. En bias hjelper med å gi en verdi som er foretrukket. \n",
    "```\n",
    "\tself.b1 = tf.Variable(np.random.uniform(-.1,.1,size=nh),name='Bias-1')  # First bias vector\n",
    "\tself.b2 = tf.Variable(np.random.uniform(-.1,.1,size=ios),name='Bias-2')  # Second bias vector\n",
    "```\n",
    "Selve bias variabelen er like stor som laget den skal tilhøre. Biasen blir koblet inn på laget som en addisjon til matrisemultiplikasjonen. Svaret er addert med bias. Dette gjøres da etter multiplikasjonen med vektene.\n",
    "\n",
    "![modell](https://github.com/MagnusPoppe/Tensorflow-Interface/blob/master/bias-model.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Han definerer input og target som tf.placeholder() variabler. Dette er fordi det skal mates inn verdier i disse kolonnene. \n",
    "```\n",
    "\tself.input  = tf.placeholder(tf.float64,shape=(1,ios),name='Input')\n",
    "\tself.target = tf.placeholder(tf.float64,shape=(1,ios),name='Target')\n",
    "```    \n",
    "\n",
    "- Input er inn-verdiene. Disse er data direkte fra modellen, slik som tensorflow skal tolke dem.\n",
    "- Target er de ønskede verdiene vi vil se når inputverdiene har gått igjennom nettet. Targetverdien er altså brukt til å sammenlikne med output. Under trening er target brukt til å vise hvor bra loss vi får ut. \n",
    "\n",
    "At størrelsen på de to matrisene er like er tilfeldig. Target matrisen skal være tilsvarende størrelse som output laget, siden disse skal sammenliknes. \n",
    "\n",
    "Data som blir sendt med i \"feed_dict\" blir plassert i disse to variablene. De blir matet direkte inn i par. Du vil ha en input-verdi og en ønsket output-verdi eller target-verdi. Begge disse er altså da forhåndsdefinert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To lag blir så definert. Først hidden layer. \n",
    "\n",
    "```\n",
    "\tself.hidden = tf.sigmoid( tf.matmul(self.input,  self.w1) + self.b1, name = \"Hiddens\")\n",
    "\tself.output = tf.sigmoid( tf.matmul(self.hidden, self.w2) + self.b2, name = \"Outputs\")\n",
    "```     \n",
    "\n",
    "Både hidden og output bruker sigmoid funksjonen for å trene vektene som ligger i vekt-lagene. Selve verdiene som hidden-layer og output består av er kun operatorer i tensorflow. Det er matrisemultiplikasjon: \n",
    "```\n",
    "\tforrige lag * vektmatrisen + bias.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error metoden er i dette tilfellet differansen mellom target og output. Svaret på dette kvadreres. Error funksjonen baserer seg så på å bruke en \"reduce\" metode. En metode for å gjøre om mange verdier om til en verdi. I dette tilfellet brukes median. Andre metoder kan være f.eks. avg.\n",
    "```\n",
    "        self.error = tf.reduce_mean(tf.square(self.target - self.output),name='MSE')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Han definerer så treningsalgoritmen, eller optimaliseringsalgoritmen.\n",
    "```\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\tself.trainer = optimizer.minimize(self.error,name='Backprop') \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening og valideringstesting\n",
    "Hele treningmetoden for autoencoder er i metoden under. Denne metoden inneholder også valideringstesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(self,epochs=100,test_interval=10,show_interval=50):\n",
    "        errors = []\n",
    "        if test_interval: self.avg_vector_distances = []\n",
    "        self.current_session = sess = TFT.gen_initialized_session()\n",
    "        step = 0\n",
    "        for i in range(epochs):\n",
    "            error = 0\n",
    "            grabvars = [self.error]\n",
    "            for c in self.cases:\n",
    "                feeder = {self.input: [c[0]], self.target: [c[1]]}\n",
    "                _,grabvals,_ = self.run_one_step([self.trainer],grabvars,step=step,show_interval=show_interval,\n",
    "                                                 session=sess,feed_dict=feeder)\n",
    "                error += grabvals[0]\n",
    "                step += 1\n",
    "            errors.append(error)\n",
    "            if (test_interval and i % test_interval == 0):\n",
    "                self.avg_vector_distances.append(calc_avg_vect_dist(self.do_testing(sess,scatter=False)))\n",
    "        PLT.figure()\n",
    "        TFT.simple_plot(errors,xtitle=\"Epoch\",ytitle=\"Error\",title=\"\")\n",
    "        if test_interval:\n",
    "            PLT.figure()\n",
    "            TFT.simple_plot(self.avg_vector_distances,xtitle='Epoch',\n",
    "                              ytitle='Avg Hidden-Node Vector Distance',title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoden begynner med å definere verdier han ønsker å lagre. Listen __\"Errors\"__ er en liste over hvordan loss funksjonen ser ut for hvert \"timestep\". Listen __\"avg_vector_distances()\"__ holder informasjon om alle runder med valideringstesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-58ca1810eb2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgrabvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfeeder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    error = 0\n",
    "    grabvars = [self.error]\n",
    "    for c in self.cases:\n",
    "        feeder = {self.input: [c[0]], self.target: [c[1]]}\n",
    "        _,grabvals,_ = self.run_one_step([self.trainer],grabvars,step=step,show_interval=show_interval,\n",
    "                                         session=sess,feed_dict=feeder)\n",
    "        error += grabvals[0]\n",
    "        step += 1\n",
    "    errors.append(error)\n",
    "    if (test_interval and i % test_interval == 0):\n",
    "        self.avg_vector_distances.append(calc_avg_vect_dist(self.do_testing(sess,scatter=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den ytre for-løkken over går igjennom epoker. En epoke er en full gjennomgang av datasettet. Her setter han erroren og definerer at han skal ha den ut av session.run() etter metoden har blitt kalt. Videre kommer en ny for løkke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8fe16c3ea3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfeeder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     _,grabvals,_ = self.run_one_step([self.trainer],grabvars,step=step,show_interval=show_interval,\n\u001b[1;32m      4\u001b[0m                                      session=sess,feed_dict=feeder)\n\u001b[1;32m      5\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrabvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for c in self.cases:\n",
    "    feeder = {self.input: [c[0]], self.target: [c[1]]}\n",
    "    _,grabvals,_ = self.run_one_step([self.trainer],grabvars,step=step,show_interval=show_interval,\n",
    "                                     session=sess,feed_dict=feeder)\n",
    "    error += grabvals[0]\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I den indre for-løkken kjøres hver mini-batch. I dette tilfellet ser vi at han kun har 1 verdi per mini-batch. Dette blir da ét case per runde i løkken. \n",
    "\n",
    "Case-verdiene sendes inn i en feeder-dictionary som senere blir brukt med session.run() sin feed_dict. Traineren kjøres en runde med variablene som er blitt oppgitt. Ut kommer de ønskede \"grab vals\". \n",
    "\n",
    "Videre kommer kallet på __run_one_step()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_step(self,operators, grabbed_vars=None, dir='probeview',\n",
    "              session=None, feed_dict=None, step=1, show_interval=1):\n",
    "    sess = session if session else TFT.gen_initialized_session(dir=dir)\n",
    "\n",
    "    results = sess.run([operators, grabbed_vars], feed_dict=feed_dict)\n",
    "    if show_interval and (step % show_interval == 0):\n",
    "        TFT.show_results(results[1], grabbed_vars, dir)\n",
    "    return results[0], results[1], sess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoden sjekker først om den allerede har en session. Hvis dette ikke er tilfellet, blir en ny session laget. Session.run() blir så kalt med de ønskede verdiene. Resten av koden er for visualisering. Verdiene returneres. \n",
    "\n",
    "Legg merke til at session også returneres. Dette er fordi session skal brukes om igjen i neste runde av løkken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tilbake i \"do_training()\" er vi nå ute av den indre løkken. __Error__ legges til i listen over alle errors. Det følgende som skjer er valideringstestingen. Her kalles \"do-testing()\" metoden, men kun om det er ønsket at testing skal skje. Resultatet fra testingen lagres i listen __\"avg_vector_distances()\"__.\n",
    "\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This particular testing is ONLY called during training, so it always receives an open session.\n",
    "def do_testing(self,session=None,scatter=True):\n",
    "    sess = session if session else self.current_session\n",
    "    hidden_activations = []\n",
    "    grabvars = [self.hidden]\n",
    "    \n",
    "    for c in self.cases:\n",
    "        feeder = {self.input: [c[0]]}\n",
    "        _,grabvals,_ = self.run_one_step([self.predictor],grabvars,session=sess,\n",
    "                                         feed_dict = feeder,show_interval=None)\n",
    "        hidden_activations.append(grabvals[0][0])\n",
    "        \n",
    "    if scatter:\n",
    "        PLT.figure()\n",
    "        vs = hidden_activations if self.num_hiddens > 3 else TFT.pca(hidden_activations,2)\n",
    "        TFT.simple_scatter_plot(hidden_activations,radius=8)\n",
    "        \n",
    "    return hidden_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\"do_testing()\"__ er veldig lik \"do-training\". Denne er litt enklere. I testing metoden er det det gjemte laget vi ser på. Vi kjører kun operatoren __\"self.predictor\"__. Dette er da en kopi av verdien til output. Testene kjøres for 1 epoke, altså alle casene, 1 gang. \n",
    "\n",
    "\"if scatter\" delen er kun grafikk. Ikke vikitg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}