{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as PLT\n",
    "import tflowtools as TFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Autoencoder ****\n",
    "# We can extend the basic approach in tfex8 (tutor1.py) to handle a) a 3-layered neural network, and b) a collection\n",
    "# of cases to be learned.  This is a specialized neural network designed to solve one type of classification\n",
    "#  problem: converting an input string, through a single hidden layer, to a copy of itself on the output end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder():\n",
    "\n",
    "    # nh = # hidden nodes (in the single hidden layer)\n",
    "    # lr = learning rate\n",
    "\n",
    "    def __init__(self,nh=3,lr=.1):\n",
    "        self.cases = TFT.gen_all_one_hot_cases(2**nh)\n",
    "        self.learning_rate = lr\n",
    "        self.num_hiddens = nh\n",
    "        self.build_neural_network(nh)\n",
    "\n",
    "    def build_neural_network(self,nh):\n",
    "        ios = 2**nh  # ios = input- and output-layer size\n",
    "        self.w1 = tf.Variable(np.random.uniform(-.1,.1,size=(ios,nh)),name='Weights-1')  # first weight array\n",
    "        self.w2 = tf.Variable(np.random.uniform(-.1,.1,size=(nh,ios)),name='Weights-2') # second weight array\n",
    "        self.b1 = tf.Variable(np.random.uniform(-.1,.1,size=nh),name='Bias-1')  # First bias vector\n",
    "        self.b2 = tf.Variable(np.random.uniform(-.1,.1,size=ios),name='Bias-2')  # Second bias vector\n",
    "        self.input = tf.placeholder(tf.float64,shape=(1,ios),name='Input')\n",
    "        self.target = tf.placeholder(tf.float64,shape=(1,ios),name='Target')\n",
    "        self.hidden = tf.sigmoid(tf.matmul(self.input,self.w1) + self.b1,name=\"Hiddens\")\n",
    "        self.output = tf.sigmoid(tf.matmul(self.hidden,self.w2) + self.b2, name = \"Outputs\")\n",
    "        self.error = tf.reduce_mean(tf.square(self.target - self.output),name='MSE')\n",
    "        self.predictor = self.output  # Simple prediction runs will request the value of outputs\n",
    "        # Defining the training operator\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.trainer = optimizer.minimize(self.error,name='Backprop')\n",
    "\n",
    "    #  This is the same as quickrun in tutorial # 1, but now it's a method, not a function.\n",
    "\n",
    "    def run_one_step(self,operators, grabbed_vars=None, dir='probeview',\n",
    "                  session=None, feed_dict=None, step=1, show_interval=1):\n",
    "        sess = session if session else TFT.gen_initialized_session(dir=dir)\n",
    "\n",
    "        results = sess.run([operators, grabbed_vars], feed_dict=feed_dict)\n",
    "        if show_interval and (step % show_interval == 0):\n",
    "            TFT.show_results(results[1], grabbed_vars, dir)\n",
    "        return results[0], results[1], sess\n",
    "\n",
    "\n",
    "    def do_training(self,epochs=100,test_interval=10,show_interval=50):\n",
    "        errors = []\n",
    "        if test_interval: self.avg_vector_distances = []\n",
    "        self.current_session = sess = TFT.gen_initialized_session()\n",
    "        step = 0\n",
    "        for i in range(epochs):\n",
    "            error = 0\n",
    "            grabvars = [self.error]\n",
    "            for c in self.cases:\n",
    "                feeder = {self.input: [c[0]], self.target: [c[1]]}\n",
    "                _,grabvals,_ = self.run_one_step([self.trainer],grabvars,step=step,show_interval=show_interval,\n",
    "                                                 session=sess,feed_dict=feeder)\n",
    "                error += grabvals[0]\n",
    "                step += 1\n",
    "            errors.append(error)\n",
    "            if (test_interval and i % test_interval == 0):\n",
    "                self.avg_vector_distances.append(calc_avg_vect_dist(self.do_testing(sess,scatter=False)))\n",
    "        PLT.figure()\n",
    "        TFT.simple_plot(errors,xtitle=\"Epoch\",ytitle=\"Error\",title=\"\")\n",
    "        if test_interval:\n",
    "            PLT.figure()\n",
    "            TFT.simple_plot(self.avg_vector_distances,xtitle='Epoch',\n",
    "                              ytitle='Avg Hidden-Node Vector Distance',title='')\n",
    "\n",
    "\n",
    "    # This particular testing is ONLY called during training, so it always receives an open session.\n",
    "    def do_testing(self,session=None,scatter=True):\n",
    "        sess = session if session else self.current_session\n",
    "        hidden_activations = []\n",
    "        grabvars = [self.hidden]\n",
    "        for c in self.cases:\n",
    "            feeder = {self.input: [c[0]]}\n",
    "            _,grabvals,_ = self.run_one_step([self.predictor],grabvars,session=sess,\n",
    "                                             feed_dict = feeder,show_interval=None)\n",
    "            hidden_activations.append(grabvals[0][0])\n",
    "        if scatter:\n",
    "            PLT.figure()\n",
    "            vs = hidden_activations if self.num_hiddens > 3 else TFT.pca(hidden_activations,2)\n",
    "            TFT.simple_scatter_plot(hidden_activations,radius=8)\n",
    "        return hidden_activations\n",
    "\n",
    "# ********  Auxiliary functions for the autoencoder example *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_distance(vect1, vect2):\n",
    "    return (sum([(v1 - v2) ** 2 for v1, v2 in zip(vect1, vect2)])) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_vect_dist(vectors):\n",
    "    n = len(vectors);\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sum += vector_distance(vectors[i], vectors[j])\n",
    "    return 2 * sum / (n * (n - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A test of the autoencoder\n",
    "def autoex1(epochs=2000,num_bits=3,lrate=0.5,tint=25,showint=100):\n",
    "    ann = autoencoder(nh=num_bits,lr=lrate)\n",
    "    PLT.ion()\n",
    "    ann.do_training(epochs,test_interval=tint,show_interval=showint)\n",
    "    ann.do_testing(scatter=True)  # Do a final round of testing to plot the hidden-layer activation vectors.\n",
    "    PLT.ioff()\n",
    "    TFT.close_session(ann.current_session)\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}