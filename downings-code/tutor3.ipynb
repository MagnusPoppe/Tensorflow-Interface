{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tflowtools'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd8a96c3713a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPLT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflowtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tflowtools'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as PLT\n",
    "import tflowtools as TFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A General Artificial Neural Network\n",
    "This is the original GANN, which has been improved in the file gann.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gann():\n",
    "\n",
    "    def __init__(self, dims, cman,lrate=.1,showint=None,mbs=10,vint=None,softmax=False):\n",
    "        self.learning_rate = lrate\n",
    "        self.layer_sizes = dims # Sizes of each layer of neurons\n",
    "        self.show_interval = showint # Frequency of showing grabbed variables\n",
    "        self.global_training_step = 0 # Enables coherent data-storage during extra training runs (see runmore).\n",
    "        self.grabvars = []  # Variables to be monitored (by gann code) during a run.\n",
    "        self.grabvar_figures = [] # One matplotlib figure for each grabvar\n",
    "        self.minibatch_size = mbs\n",
    "        self.validation_interval = vint\n",
    "        self.validation_history = []\n",
    "        self.caseman = cman\n",
    "        self.softmax_outputs = softmax\n",
    "        self.modules = []\n",
    "        self.build()\n",
    "\n",
    "    # Probed variables are to be displayed in the Tensorboard.\n",
    "    def gen_probe(self, module_index, type, spec):\n",
    "        self.modules[module_index].gen_probe(type,spec)\n",
    "\n",
    "    # Grabvars are displayed by my own code, so I have more control over the display format.  Each\n",
    "    # grabvar gets its own matplotlib figure in which to display its value.\n",
    "    def add_grabvar(self,module_index,type='wgt'):\n",
    "        self.grabvars.append(self.modules[module_index].getvar(type))\n",
    "        self.grabvar_figures.append(PLT.figure())\n",
    "\n",
    "    def roundup_probes(self):\n",
    "        self.probes = tf.summary.merge_all()\n",
    "\n",
    "    def add_module(self,module): self.modules.append(module)\n",
    "\n",
    "    def build(self):\n",
    "        tf.reset_default_graph()  # This is essential for doing multiple runs!!\n",
    "        num_inputs = self.layer_sizes[0]\n",
    "        self.input = tf.placeholder(tf.float64, shape=(None, num_inputs), name='Input')\n",
    "        invar = self.input; insize = num_inputs\n",
    "        # Build all of the modules\n",
    "        for i,outsize in enumerate(self.layer_sizes[1:]):\n",
    "            gmod = Gannmodule(self,i,invar,insize,outsize)\n",
    "            invar = gmod.output; insize = gmod.outsize\n",
    "        self.output = gmod.output # Output of last module is output of whole network\n",
    "        if self.softmax_outputs: self.output = tf.nn.softmax(self.output)\n",
    "        self.target = tf.placeholder(tf.float64,shape=(None,gmod.outsize),name='Target')\n",
    "        self.configure_learning()\n",
    "\n",
    "    # The optimizer knows to gather up all \"trainable\" variables in the function graph and compute\n",
    "    # derivatives of the error function with respect to each component of each variable, i.e. each weight\n",
    "    # of the weight array.\n",
    "\n",
    "    def configure_learning(self):\n",
    "        self.error = tf.reduce_mean(tf.square(self.target - self.output),name='MSE')\n",
    "        self.predictor = self.output  # Simple prediction runs will request the value of output neurons\n",
    "        # Defining the training operator\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.trainer = optimizer.minimize(self.error,name='Backprop')\n",
    "\n",
    "    def do_training(self,sess,cases,epochs=100,continued=False):\n",
    "        if not(continued): self.error_history = []\n",
    "        for i in range(epochs):\n",
    "            error = 0; step = self.global_training_step + i\n",
    "            gvars = [self.error] + self.grabvars\n",
    "            mbs = self.minibatch_size; ncases = len(cases); nmb = math.ceil(ncases/mbs)\n",
    "            for cstart in range(0,ncases,mbs):  # Loop through cases, one minibatch at a time.\n",
    "                cend = min(ncases,cstart+mbs)\n",
    "                minibatch = cases[cstart:cend]\n",
    "                inputs = [c[0] for c in minibatch]; targets = [c[1] for c in minibatch]\n",
    "                feeder = {self.input: inputs, self.target: targets}\n",
    "                _,grabvals,_ = self.run_one_step([self.trainer],gvars,self.probes,session=sess,\n",
    "                                         feed_dict=feeder,step=step,show_interval=self.show_interval)\n",
    "                error += grabvals[0]\n",
    "            self.error_history.append((step, error/nmb))\n",
    "            self.consider_validation_testing(step,sess)\n",
    "        self.global_training_step += epochs\n",
    "        TFT.plot_training_history(self.error_history,self.validation_history,xtitle=\"Epoch\",ytitle=\"Error\",\n",
    "                                  title=\"\",fig=not(continued))\n",
    "\n",
    "    def do_testing(self,sess,cases,msg='Testing'):\n",
    "        inputs = [c[0] for c in cases]; targets = [c[1] for c in cases]\n",
    "        feeder = {self.input: inputs, self.target: targets}\n",
    "        error, grabvals, _ = self.run_one_step(self.error, self.grabvars, self.probes, session=sess,\n",
    "                                           feed_dict=feeder,  show_interval=None)\n",
    "        print('%s Set Error = %f ' % (msg, error))\n",
    "        return error  # self.error uses MSE, so this is a per-case value\n",
    "\n",
    "\n",
    "    def training_session(self,epochs,sess=None,dir=\"probeview\",continued=False):\n",
    "        self.roundup_probes()\n",
    "        session = sess if sess else TFT.gen_initialized_session(dir=dir)\n",
    "        self.current_session = session\n",
    "        self.do_training(session,self.caseman.get_training_cases(),epochs,continued=continued)\n",
    "\n",
    "    def testing_session(self,sess):\n",
    "        cases = self.caseman.get_testing_cases()\n",
    "        if len(cases) > 0:\n",
    "            self.do_testing(sess,cases,msg='Final Testing')\n",
    "\n",
    "    def consider_validation_testing(self,epoch,sess):\n",
    "        if self.validation_interval and (epoch % self.validation_interval == 0):\n",
    "            cases = self.caseman.get_validation_cases()\n",
    "            if len(cases) > 0:\n",
    "                error = self.do_testing(sess,cases,msg='Validation Testing')\n",
    "                self.validation_history.append((epoch,error))\n",
    "\n",
    "    # Do testing (i.e. calc error without learning) on the training set.\n",
    "    def test_on_trains(self,sess):\n",
    "        self.do_testing(sess,self.caseman.get_training_cases(),msg='Total Training')\n",
    "\n",
    "    # Similar to the \"quickrun\" functions used earlier.\n",
    "\n",
    "    def run_one_step(self, operators, grabbed_vars=None, probed_vars=None, dir='probeview',\n",
    "                  session=None, feed_dict=None, step=1, show_interval=1):\n",
    "        sess = session if session else TFT.gen_initialized_session(dir=dir)\n",
    "        if probed_vars is not None:\n",
    "            results = sess.run([operators, grabbed_vars, probed_vars], feed_dict=feed_dict)\n",
    "            sess.probe_stream.add_summary(results[2], global_step=step)\n",
    "        else:\n",
    "            results = sess.run([operators, grabbed_vars], feed_dict=feed_dict)\n",
    "        if show_interval and (step % show_interval == 0):\n",
    "            self.display_grabvars(results[1], grabbed_vars, step=step)\n",
    "        return results[0], results[1], sess\n",
    "\n",
    "    def display_grabvars(self, grabbed_vals, grabbed_vars,step=1):\n",
    "        names = [x.name for x in grabbed_vars];\n",
    "        msg = \"Grabbed Variables at Step \" + str(step)\n",
    "        print(\"\\n\" + msg, end=\"\\n\")\n",
    "        fig_index = 0\n",
    "        for i, v in enumerate(grabbed_vals):\n",
    "            if names: print(\"   \" + names[i] + \" = \", end=\"\\n\")\n",
    "            if type(v) == np.ndarray and len(v.shape) > 1: # If v is a matrix, use hinton plotting\n",
    "                TFT.hinton_plot(v,fig=self.grabvar_figures[fig_index],title= names[i]+ ' at step '+ str(step))\n",
    "                fig_index += 1\n",
    "            else:\n",
    "                print(v, end=\"\\n\\n\")\n",
    "\n",
    "    def run(self,epochs=100,sess=None,continued=False):\n",
    "        PLT.ion()\n",
    "        self.training_session(epochs,sess=sess,continued=continued)\n",
    "        self.test_on_trains(sess=self.current_session)\n",
    "        self.testing_session(sess=self.current_session)\n",
    "        self.close_current_session()\n",
    "        PLT.ioff()\n",
    "\n",
    "    # After a run is complete, runmore allows us to do additional training on the network, picking up where we\n",
    "    # left off after the last call to run (or runmore).  Use of the \"continued\" parameter (along with\n",
    "    # global_training_step) allows easy updating of the error graph to account for the additional run(s).\n",
    "\n",
    "    def runmore(self,epochs=100):\n",
    "        self.reopen_current_session()\n",
    "        self.run(epochs,sess=self.current_session,continued=True)\n",
    "\n",
    "    #   ******* Saving GANN Parameters (weights and biases) *******************\n",
    "    # This is useful when you want to use \"runmore\" to do additional training on a network.\n",
    "    # spath should have at least one directory (e.g. netsaver), which you will need to create ahead of time.\n",
    "    # This is also useful for situations where you want to first train the network, then save its parameters\n",
    "    # (i.e. weights and biases), and then run the trained network on a set of test cases where you may choose to\n",
    "    # monitor the network's activity (via grabvars, probes, etc) in a different way than you monitored during\n",
    "    # training.\n",
    "\n",
    "    def save_session_params(self, spath='netsaver/my_saved_session', sess=None, step=0):\n",
    "        session = sess if sess else self.current_session\n",
    "        state_vars = []\n",
    "        for m in self.modules:\n",
    "            vars = [m.getvar('wgt'), m.getvar('bias')]\n",
    "            state_vars = state_vars + vars\n",
    "        self.state_saver = tf.train.Saver(state_vars)\n",
    "        self.saved_state_path = self.state_saver.save(session, spath, global_step=step)\n",
    "\n",
    "    def reopen_current_session(self):\n",
    "        self.current_session = TFT.copy_session(self.current_session)  # Open a new session with same tensorboard stuff\n",
    "        self.current_session.run(tf.global_variables_initializer())\n",
    "        self.restore_session_params()  # Reload old weights and biases to continued from where we last left off\n",
    "\n",
    "    def restore_session_params(self, path=None, sess=None):\n",
    "        spath = path if path else self.saved_state_path\n",
    "        session = sess if sess else self.current_session\n",
    "        self.state_saver.restore(session, spath)\n",
    "\n",
    "    def close_current_session(self):\n",
    "        self.save_session_params(sess=self.current_session)\n",
    "        TFT.close_session(self.current_session, view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general ann module = a layer of neurons (the output) plus its incoming weights and biases.\n",
    "class Gannmodule():\n",
    "\n",
    "    def __init__(self,ann,index,invariable,insize,outsize):\n",
    "        self.ann = ann\n",
    "        self.insize=insize  # Number of neurons feeding into this module\n",
    "        self.outsize=outsize # Number of neurons in this module\n",
    "        self.input = invariable  # Either the gann's input variable or the upstream module's output\n",
    "        self.index = index\n",
    "        self.name = \"Module-\"+str(self.index)\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        mona = self.name; n = self.outsize\n",
    "        self.weights = tf.Variable(np.random.uniform(-.1, .1, size=(self.insize,n)),\n",
    "                                   name=mona+'-wgt',trainable=True) # True = default for trainable anyway\n",
    "        self.biases = tf.Variable(np.random.uniform(-.1, .1, size=n),\n",
    "                                  name=mona+'-bias', trainable=True)  # First bias vector\n",
    "        self.output = tf.nn.relu(tf.matmul(self.input,self.weights)+self.biases,name=mona+'-out')\n",
    "        self.ann.add_module(self)\n",
    "\n",
    "    def getvar(self,type):  # type = (in,out,wgt,bias)\n",
    "        return {'in': self.input, 'out': self.output, 'wgt': self.weights, 'bias': self.biases}[type]\n",
    "\n",
    "    # spec, a list, can contain one or more of (avg,max,min,hist); type = (in, out, wgt, bias)\n",
    "    def gen_probe(self,type,spec):\n",
    "        var = self.getvar(type)\n",
    "        base = self.name +'_'+type\n",
    "        with tf.name_scope('probe_'):\n",
    "            if ('avg' in spec) or ('stdev' in spec):\n",
    "                avg = tf.reduce_mean(var)\n",
    "            if 'avg' in spec:\n",
    "                tf.summary.scalar(base + '/avg/', avg)\n",
    "            if 'max' in spec:\n",
    "                tf.summary.scalar(base + '/max/', tf.reduce_max(var))\n",
    "            if 'min' in spec:\n",
    "                tf.summary.scalar(base + '/min/', tf.reduce_min(var))\n",
    "            if 'hist' in spec:\n",
    "                tf.summary.histogram(base + '/hist/',var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE MANAGER\n",
    "This is a simple class for organizing the cases (training, validation and test) for a a machine-learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caseman():\n",
    "\n",
    "    def __init__(self,cfunc,vfrac=0,tfrac=0):\n",
    "        self.casefunc = cfunc\n",
    "        self.validation_fraction = vfrac\n",
    "        self.test_fraction = tfrac\n",
    "        self.training_fraction = 1 - (vfrac + tfrac)\n",
    "        self.generate_cases()\n",
    "        self.organize_cases()\n",
    "\n",
    "    def generate_cases(self):\n",
    "        self.cases = self.casefunc()  # Run the case generator.  Case = [input-vector, target-vector]\n",
    "\n",
    "    def organize_cases(self):\n",
    "        ca = np.array(self.cases)\n",
    "        np.random.shuffle(ca) # Randomly shuffle all cases\n",
    "        separator1 = round(len(self.cases) * self.training_fraction)\n",
    "        separator2 = separator1 + round(len(self.cases)*self.validation_fraction)\n",
    "        self.training_cases = ca[0:separator1]\n",
    "        self.validation_cases = ca[separator1:separator2]\n",
    "        self.testing_cases = ca[separator2:]\n",
    "\n",
    "    def get_training_cases(self): return self.training_cases\n",
    "    def get_validation_cases(self): return self.validation_cases\n",
    "    def get_testing_cases(self): return self.testing_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN functions\n",
    "After running this, open a Tensorboard (Go to localhost:6006 in your Chrome Browser) and check the 'scalar', 'distribution' and 'histogram' menu options to view the probed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoex(epochs=300,nbits=4,lrate=0.03,showint=100,mbs=None,vfrac=0.1,tfrac=0.1,vint=100,sm=False):\n",
    "    size = 2**nbits\n",
    "    mbs = mbs if mbs else size\n",
    "    case_generator = (lambda : TFT.gen_all_one_hot_cases(2**nbits))\n",
    "    cman = Caseman(cfunc=case_generator,vfrac=vfrac,tfrac=tfrac)\n",
    "    ann = Gann(dims=[size,nbits,size],cman=cman,lrate=lrate,showint=showint,mbs=mbs,vint=vint,softmax=sm)\n",
    "    ann.gen_probe(0,'wgt',('hist','avg'))  # Plot a histogram and avg of the incoming weights to module 0.\n",
    "    ann.gen_probe(1,'out',('avg','max'))  # Plot average and max value of module 1's output vector\n",
    "    ann.add_grabvar(0,'wgt') # Add a grabvar (to be displayed in its own matplotlib window).\n",
    "    ann.run(epochs)\n",
    "    ann.runmore(epochs*2)\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}